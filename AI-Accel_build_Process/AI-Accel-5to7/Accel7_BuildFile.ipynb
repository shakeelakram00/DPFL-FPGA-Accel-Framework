{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install brevitas\n",
    "!pip install -U netron\n",
    "\n",
    "# PyTorch libraries and modules\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.utils import resample\n",
    "\n",
    "from torch.nn import Module, ModuleList, BatchNorm2d, MaxPool2d, BatchNorm1d, ReLU, Softmax, CrossEntropyLoss, Sequential, Dropout, Conv2d, Linear\n",
    "\n",
    "from brevitas.nn import QuantConv2d, QuantIdentity, QuantLinear, QuantReLU\n",
    "from brevitas.core.restrict_val import RestrictValueType\n",
    "from tensor_norm import TensorNorm\n",
    "from common import CommonWeightQuant, CommonActQuant\n",
    "\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.custom_op.registry import getCustomOp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNV_OUT_CH_POOL = [(21, False), (21, True), (21, False)] #[(21, False), (21, True), (21, False)]\n",
    "INTERMEDIATE_FC_FEATURES = [(3549, 16), (16, 16)] #[(3549, 16), (16, 16)]\n",
    "LAST_FC_IN_FEATURES = 16\n",
    "LAST_FC_PER_OUT_CH_SCALING = False\n",
    "POOL_SIZE = 2\n",
    "KERNEL_SIZE = 6\n",
    "\n",
    "MixPrecisionBits = 4  \n",
    "ConvPrecisonBits = 4  \n",
    "LinearPrecisonBits = 4\n",
    "\n",
    "class CNV(Module):\n",
    "\n",
    "    def __init__(self, num_classes, weight_bit_width, act_bit_width, in_bit_width, in_ch):\n",
    "        super(CNV, self).__init__()\n",
    "\n",
    "        self.conv_features = ModuleList()\n",
    "        self.linear_features = ModuleList()\n",
    "\n",
    "        self.conv_features.append(QuantIdentity( # for Q1.7 input format\n",
    "            act_quant=CommonActQuant,\n",
    "            bit_width=in_bit_width,\n",
    "            min_val=- 1.0,\n",
    "            max_val=1.0 - 2.0 ** (-7),\n",
    "            narrow_range=True,  ###If True implements the value in a range he range for weights and biases\n",
    "            #will be from -2^(N-1) + 1 to 2^(N-1), where N is the bit width. This is different from the default \n",
    "            #range of -2^(N-1) to 2^(N-1) when narrow_range is False.\n",
    "            #narrow_range = True makes the hardware inference more efficient\n",
    "            restrict_scaling_type=RestrictValueType.POWER_OF_TWO))\n",
    "\n",
    "        for out_ch, is_pool_enabled in CNV_OUT_CH_POOL:\n",
    "            self.conv_features.append(QuantConv2d(kernel_size=KERNEL_SIZE, in_channels=in_ch, out_channels=out_ch,\n",
    "                bias=True, padding=4, weight_quant=CommonWeightQuant, weight_bit_width=ConvPrecisonBits))#made bias=False\n",
    "            in_ch = out_ch\n",
    "            self.conv_features.append(BatchNorm2d(in_ch, eps=1e-4))\n",
    "            self.conv_features.append(QuantIdentity(act_quant=CommonActQuant,bit_width=MixPrecisionBits))#MultiThreshold123\n",
    "            if is_pool_enabled:\n",
    "                self.conv_features.append(MaxPool2d(kernel_size=2))\n",
    "\n",
    "        for in_features, out_features in INTERMEDIATE_FC_FEATURES:\n",
    "            self.linear_features.append(QuantLinear(in_features=in_features, out_features=out_features, bias=True,\n",
    "                weight_quant=CommonWeightQuant, weight_bit_width=LinearPrecisonBits))\n",
    "            self.linear_features.append(BatchNorm1d(out_features, eps=1e-4))\n",
    "            self.linear_features.append(QuantIdentity(act_quant=CommonActQuant,bit_width=LinearPrecisonBits))#MultiThreshold45\n",
    "\n",
    "        self.linear_features.append(QuantLinear(in_features=LAST_FC_IN_FEATURES, out_features=num_classes, bias=False,\n",
    "            weight_quant=CommonWeightQuant, weight_bit_width=LinearPrecisonBits))\n",
    "        self.linear_features.append(TensorNorm())\n",
    "        \n",
    "        for m in self.modules():\n",
    "            if isinstance(m, QuantConv2d) or isinstance(m, QuantLinear):\n",
    "                torch.nn.init.uniform_(m.weight.data, -1, 1)\n",
    "                #print(f\"Weight Data Convolution {m+1}\", m.weight.data)\n",
    "\n",
    "\n",
    "    def clip_weights(self, min_val, max_val):\n",
    "        for mod in self.conv_features:\n",
    "            if isinstance(mod, QuantConv2d):\n",
    "                mod.weight.data.clamp_(min_val, max_val)\n",
    "                #print(f\"Weight Data Convolution {mod+1}\", mod.weight.data)\n",
    "        for mod in self.linear_features:\n",
    "            if isinstance(mod, QuantLinear):\n",
    "                mod.weight.data.clamp_(min_val, max_val)\n",
    "                #print(f\"Weight Data Convolution {mod+1}\", mod.weight.data)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"Data Feeded:\",x)\n",
    "        x = 2.0 * x - torch.tensor([1.0], device=x.device)\n",
    "        #print(\"Data Processesd(2x-1):\",x)\n",
    "        for mod in self.conv_features:\n",
    "            x = mod(x)\n",
    "            #print(f\"Data After Convolution Feature {mod+1}:\",x)\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        #print(\"Data After Flatten:\",x)\n",
    "        for mod in self.linear_features:\n",
    "            x = mod(x)\n",
    "            #print(f\"Data After Linear Feature {mod+1}:\",x)\n",
    "        return x\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CNV(num_classes=5, weight_bit_width=1, act_bit_width=1, in_bit_width=8, in_ch=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_reshape = torch.load(\"xtrain25000014x14float32.pth\")\n",
    "ytrain_tensor = torch.load(\"ytrain250000int64.pth\")\n",
    "xval_reshape = torch.load(\"xval4412014x14float32.pth\")\n",
    "yval_tensor = torch.load(\"yval44120int64.pth\")\n",
    "xtest_reshape = torch.load(\"xtest4527014x14float32.pth\")\n",
    "ytest_tensor = torch.load(\"ytest45270int64.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X.unsqueeze(1)\n",
    "        self.y = y\n",
    "        self.len = self.X.shape[0]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "train_data = Data(xtrain_reshape, ytrain_tensor)\n",
    "train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "val_data = Data(xval_reshape, yval_tensor)\n",
    "val_dataloader = DataLoader(dataset=val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_data = Data(xtest_reshape, ytest_tensor)\n",
    "test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# # Check it's working\n",
    "# for batch, (X, y) in enumerate(train_dataloader):\n",
    "#     print(f\"Batch: {batch+1}\")\n",
    "#     print(f\"XTrain shape: {X.shape}\")\n",
    "#     print(f\"yTrain shape: {y.shape}\")\n",
    "#     break\n",
    "# for batch, (X, y) in enumerate(val_dataloader):\n",
    "#     print(f\"Batch: {batch+1}\")\n",
    "#     print(f\"XVal: {X.shape}\")\n",
    "#     print(f\"yVal: {y.shape}\")\n",
    "#     break\n",
    "# for batch, (X, y) in enumerate(test_dataloader):\n",
    "#     print(f\"Batch: {batch+1}\")\n",
    "#     print(f\"XTest: {X.shape}\")\n",
    "#     print(f\"yTest: {y.shape}\")\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = CrossEntropyLoss()\n",
    "local_ep = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "epoch_loss = []\n",
    "batch_loss = []\n",
    "\n",
    "for iter in range(local_ep):\n",
    "    model.train()\n",
    "    criterion.train()\n",
    "    \n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "    train_batch_loss =[]\n",
    "    \n",
    "    total_batches = len(train_dataloader)\n",
    "    \n",
    "    progress_bar = tqdm(total=total_batches, desc=\"Processing Local Epoch(s)\", unit=\"batch\", position=0, leave=True)\n",
    "    \n",
    "    for batch_idx, (xtrain, ytrain) in enumerate(train_dataloader):\n",
    "        xtrain, ytrain = xtrain.to(device), ytrain.to(device)\n",
    "        model_preds = model(xtrain)\n",
    "        _, pred_labels = torch.max(model_preds, 1)\n",
    "        train_correct += torch.sum(pred_labels == ytrain).item()\n",
    "        train_total += ytrain.size(0)\n",
    "        loss = criterion(model_preds, ytrain)\n",
    "        train_batch_loss.append(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        model.clip_weights(-1, 1)\n",
    "        \n",
    "        batch_loss.append(loss.item())\n",
    "                \n",
    "        progress_bar.update(1)\n",
    "        progress_bar.set_postfix(batch=batch_idx + 1, refresh=True)\n",
    "        \n",
    "    progress_bar.close()\n",
    "    \n",
    "    epoch_loss.append(sum(batch_loss)/len(batch_loss))\n",
    "    \n",
    "    model.eval() \n",
    "    criterion.eval()\n",
    "    \n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "    val_batch_loss = []       \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (xval, yval) in enumerate(val_dataloader):\n",
    "            xval, yval = xval.to(device), yval.to(device)\n",
    "            val_model_preds = model(xval)\n",
    "            val_loss = criterion(val_model_preds, yval)\n",
    "            val_batch_loss.append(val_loss.item())\n",
    "            _, val_pred_labels = torch.max(val_model_preds, 1)\n",
    "            val_correct += torch.sum(val_pred_labels == yval).item()\n",
    "            val_total += yval.size(0)\n",
    "    \n",
    "    # Calculate and print average training and validation losses and accuracies\n",
    "    avg_train_loss = sum(train_batch_loss) / len(train_batch_loss)\n",
    "    train_accuracy = train_correct / train_total if train_total > 0 else 0.0\n",
    "    avg_val_loss = sum(val_batch_loss) / len(val_batch_loss)\n",
    "    val_accuracy = val_correct / val_total if val_total > 0 else 0.0\n",
    "    print('Local Epoch: ', iter + 1,\n",
    "          'Training Loss: ', avg_train_loss, 'Training Accuracy:{:.4f}%'.format(100*train_accuracy))\n",
    "    print('Validation Loss: ', avg_val_loss, 'Validation Accuracy:{:.4f}%'.format(100*val_accuracy))\n",
    "    model.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dir = 'Accel7/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), build_dir +'Model_State_dict.pth')\n",
    "torch.save(optimizer.state_dict(), build_dir +\"Optimizer_state_dict.pth\")\n",
    "print (\"Loss\", sum(epoch_loss) / len(epoch_loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(build_dir +'Model_State_dict.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_inference(model, test_dataloader):\n",
    "    model.eval()\n",
    "    Tall_true_labels = []\n",
    "    Tall_predicted_labels = []\n",
    "\n",
    "    loss, total, correct = 0.0, 0.0, 0.0\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    for batch_idx, (xtest, ytest) in enumerate(test_dataloader):\n",
    "\n",
    "        xtest, ytest = xtest.to(device), ytest.to(device)\n",
    "\n",
    "        outputs = model(xtest)\n",
    "        batch_loss = criterion(outputs, ytest)\n",
    "        loss += batch_loss.item()\n",
    "\n",
    "        pred = outputs.data.argmax(1, keepdim=True)\n",
    "        correct += pred.eq(ytest.data.view_as(pred)).sum()\n",
    "        total += len(ytest)\n",
    "    accuracy = 100. * correct.float() / total                    \n",
    "    loss = loss/total\n",
    "\n",
    "    return accuracy, loss\n",
    "\n",
    "test_inference(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.onnx as bo\n",
    "bo.export_finn_onnx(model, (1, 1, 14, 14), build_dir +\"export.onnx\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(build_dir + \"export.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx\n",
    "from finn.util.test import get_test_model_trained\n",
    "import brevitas.onnx as bo\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "from qonnx.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "\n",
    "model = ModelWrapper(build_dir + \"export.onnx\")\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model.save(build_dir + \"tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(build_dir + \"tidy.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.pytorch import ToTensor\n",
    "from qonnx.transformation.merge_onnx_models import MergeONNXModels\n",
    "from qonnx.core.datatype import DataType\n",
    "from qonnx.transformation.insert_topk import InsertTopK\n",
    "from qonnx.transformation.infer_datatypes import InferDataTypes\n",
    "import brevitas.onnx as bo\n",
    "from qonnx.core.modelwrapper import ModelWrapper\n",
    "\n",
    "model = ModelWrapper(build_dir+\"tidy.onnx\")\n",
    "global_inp_name = model.graph.input[0].name\n",
    "global_inp_name = model.graph.input[0].name\n",
    "ishape = model.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()                                              \n",
    "chkpt_preproc_name = build_dir+\"preproc.onnx\"\n",
    "bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model = model.transform(MergeONNXModels(pre_model))\n",
    "# # add input quantization annotation: UINT8 for all BNN-PYNQ models\n",
    "global_inp_name = model.graph.input[0].name\n",
    "model.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n",
    "\n",
    "\n",
    "from qonnx.transformation.infer_shapes import InferShapes\n",
    "from qonnx.transformation.fold_constants import FoldConstants\n",
    "\n",
    "model = model.transform(InsertTopK(k=1))\n",
    "model = model.transform(InferShapes())\n",
    "model = model.transform(FoldConstants())\n",
    "model = model.transform(InferDataTypes())\n",
    "\n",
    "\n",
    "model.save(build_dir + \"preproc2.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "showInNetron(build_dir + \"preproc2.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.builder.build_dataflow as build\n",
    "import finn.builder.build_dataflow_config as build_cfg\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "model_file = build_dir+\"preproc2.onnx\"\n",
    "\n",
    "rtlsim_output_dir = build_dir+\"Buildv2\"\n",
    "\n",
    "#Delete previous run results if exist\n",
    "if os.path.exists(rtlsim_output_dir):\n",
    "    shutil.rmtree(rtlsim_output_dir)\n",
    "    print(\"Previous run results deleted!\")\n",
    "\n",
    "cfg_stitched_ip = build.DataflowBuildConfig(\n",
    "    output_dir          = rtlsim_output_dir,\n",
    "    mvau_wwidth_max     = 10,\n",
    "    target_fps          = 1000000,\n",
    "    synth_clk_period_ns = 10.0,\n",
    "    fpga_part           = \"xczu9eg-ffvb1156-2-e\",\n",
    "    board               = \"ZCU102\",\n",
    "    shell_flow_type     = build_cfg.ShellFlowType.VIVADO_ZYNQ,\n",
    "\n",
    "    folding_config_file = build_dir+\"final_hw_config_Accel7.json\",\n",
    "\n",
    "    auto_fifo_depths    = False,\n",
    "    \n",
    "#     steps=[\"step_apply_folding_config\",\n",
    "#            \"step_generate_estimate_reports\",\n",
    "#            \"step_hls_codegen\",\n",
    "#            \"step_hls_ipgen\",\n",
    "#            \"step_set_fifo_depths\",\n",
    "#            \"step_create_stitched_ip\",\n",
    "#            \"step_measure_rtlsim_performance\",\n",
    "#            \"step_out_of_context_synthesis\",\n",
    "#            \"step_synthesize_bitfile\",\n",
    "#            \"step_make_pynq_driver\",\n",
    "#           ],\n",
    "    generate_outputs=[\n",
    "        build_cfg.DataflowOutputType.STITCHED_IP,\n",
    "        build_cfg.DataflowOutputType.RTLSIM_PERFORMANCE,\n",
    "        build_cfg.DataflowOutputType.OOC_SYNTH,\n",
    "        build_cfg.DataflowOutputType.BITFILE,\n",
    "        build_cfg.DataflowOutputType.PYNQ_DRIVER,\n",
    "        build_cfg.DataflowOutputType.DEPLOYMENT_PACKAGE]\n",
    ")\n",
    " \n",
    "\n",
    "#     auto_fifo_depths    = False,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "build.build_dataflow_cfg(model_file, cfg_stitched_ip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
